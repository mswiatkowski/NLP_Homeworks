{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "from transformers import BertForMaskedLM, BertTokenizer, pipeline, set_seed\n",
        "import pprint"
      ],
      "metadata": {
        "id": "c6k6FOhgzI0Q"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 1: Przeczytać dokumentację \"Language modelling in the Transformers\""
      ],
      "metadata": {
        "id": "FyHeilKBT5_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Przeczytana!"
      ],
      "metadata": {
        "id": "q9sHIO15ofAg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 2: Pobrać trzy polskie modele z Huggingface"
      ],
      "metadata": {
        "id": "oQU14f4uT8_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 1 - dkleczek/bert-base-polish-cased-v1\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
        "nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0tJxngSyr1t",
        "outputId": "3e680061-4a24-4c17-959b-8ffbb063144a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model 2 - xlm-roberta-base\n",
        "\n",
        "unmasker_m2 = pipeline('fill-mask', model='xlm-roberta-base')  "
      ],
      "metadata": {
        "id": "EtHi016k8okM"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 3 - distilbert-base-multilingual-cased\n",
        "\n",
        "unmasker_m3 = pipeline('fill-mask', model='distilbert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "prw62aHjHkjX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Powyższe trzy modele testowałem dla takich samych zdań. Poniżej jest papuGaPT2, który wziąłem, bo miał fantastycznie literackie wyniki i szkoda mi było go nie sprawdzić. Z powodu jego specyfiki polegającej na zdolności tylko do kończenia zdań, nie wrzuciłem do niego tych samych zdań, co do pozostałych."
      ],
      "metadata": {
        "id": "CK0vjLkxVL6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 4 - flax-community/papuGaPT2\n",
        "\n",
        "generator = pipeline('text-generation', model='flax-community/papuGaPT2')\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "GsKQV0xSOUSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 3: Sprawdzanie czy modele są w stanie uchwycić przypadki"
      ],
      "metadata": {
        "id": "4oK-XbbGUBZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mianownik:"
      ],
      "metadata": {
        "id": "0zyWB8jLUKfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Warszawa to największe polskie {nlp.tokenizer.mask_token}.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Warszawa to największe polskie <mask>.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Warszawa to największe polskie [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gbLjJhc1DOM",
        "outputId": "30c156cb-4ff7-45ce-e576-0257779a5a0d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.9310519099235535, 'token': 3866, 'token_str': 'miasto', 'sequence': 'Warszawa to największe polskie miasto.'}\n",
            "{'score': 0.014125281944870949, 'token': 2729, 'token_str': 'państwo', 'sequence': 'Warszawa to największe polskie państwo.'}\n",
            "{'score': 0.012573330663144588, 'token': 11140, 'token_str': 'lotnisko', 'sequence': 'Warszawa to największe polskie lotnisko.'}\n",
            "{'score': 0.012324074283242226, 'token': 28190, 'token_str': 'miasteczko', 'sequence': 'Warszawa to największe polskie miasteczko.'}\n",
            "{'score': 0.011786682531237602, 'token': 26810, 'token_str': 'województwo', 'sequence': 'Warszawa to największe polskie województwo.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.8516205549240112,\n",
            "  'sequence': 'Warszawa to największe polskie miasto.',\n",
            "  'token': 131020,\n",
            "  'token_str': 'miasto'},\n",
            " {'score': 0.031604476273059845,\n",
            "  'sequence': 'Warszawa to największe polskie centrum.',\n",
            "  'token': 18977,\n",
            "  'token_str': 'centrum'},\n",
            " {'score': 0.026950370520353317,\n",
            "  'sequence': 'Warszawa to największe polskie Miasto.',\n",
            "  'token': 219506,\n",
            "  'token_str': 'Miasto'},\n",
            " {'score': 0.022147636860609055,\n",
            "  'sequence': 'Warszawa to największe polskie miasta.',\n",
            "  'token': 42883,\n",
            "  'token_str': 'miasta'},\n",
            " {'score': 0.011649411171674728,\n",
            "  'sequence': 'Warszawa to największe polskie muzeum.',\n",
            "  'token': 192491,\n",
            "  'token_str': 'muzeum'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6407451629638672,\n",
              "  'token': 22208,\n",
              "  'token_str': 'miasto',\n",
              "  'sequence': 'Warszawa to największe polskie miasto.'},\n",
              " {'score': 0.08314342796802521,\n",
              "  'token': 17993,\n",
              "  'token_str': 'miasta',\n",
              "  'sequence': 'Warszawa to największe polskie miasta.'},\n",
              " {'score': 0.05527544766664505,\n",
              "  'token': 21203,\n",
              "  'token_str': 'centrum',\n",
              "  'sequence': 'Warszawa to największe polskie centrum.'},\n",
              " {'score': 0.017164424061775208,\n",
              "  'token': 54078,\n",
              "  'token_str': 'muzeum',\n",
              "  'sequence': 'Warszawa to największe polskie muzeum.'},\n",
              " {'score': 0.016599096357822418,\n",
              "  'token': 62733,\n",
              "  'token_str': 'Miasto',\n",
              "  'sequence': 'Warszawa to największe polskie Miasto.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Stolicą Polski jest')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVYshkDsOZD6",
        "outputId": "598e1daa-cf84-47d3-ab81-db41ff9959eb"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Stolicą Polski jest Warszawa. W stolicy jest ona bogata z różnych powodów, między innymi historycznych, kulturowych, architektonicznych i duchowych. W jej obrębie znajduje się wiele interesujących instytucji, których liczba również jest bardzo znacząca. Można tam podziwiać wiele zabytków, między innymi'}]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopełniacz:"
      ],
      "metadata": {
        "id": "dqpmIBZ9UNGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Czy ty naprawdę rozbiłeś samochód należący do {nlp.tokenizer.mask_token}.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Czy ty naprawdę rozbiłeś samochód należący do <mask>.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Czy ty naprawdę rozbiłeś samochód należący do [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWgNF0Ta6eK3",
        "outputId": "8e11d7dd-d77b-45c3-be1a-09e093e27d68"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.15786370635032654, 'token': 1010, 'token_str': 'mnie', 'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do mnie.'}\n",
            "{'score': 0.05596911907196045, 'token': 1624, 'token_str': 'ciebie', 'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do ciebie.'}\n",
            "{'score': 0.045427948236465454, 'token': 2202, 'token_str': 'niego', 'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do niego.'}\n",
            "{'score': 0.027413245290517807, 'token': 7215, 'token_str': 'policji', 'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do policji.'}\n",
            "{'score': 0.027012962847948074, 'token': 2486, 'token_str': 'niej', 'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do niej.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.24791032075881958,\n",
            "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do ciebie.',\n",
            "  'token': 195765,\n",
            "  'token_str': 'ciebie'},\n",
            " {'score': 0.2114931344985962,\n",
            "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do Ciebie.',\n",
            "  'token': 71546,\n",
            "  'token_str': 'Ciebie'},\n",
            " {'score': 0.05607987940311432,\n",
            "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do rodziny.',\n",
            "  'token': 89121,\n",
            "  'token_str': 'rodziny'},\n",
            " {'score': 0.03175649419426918,\n",
            "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do firmy.',\n",
            "  'token': 13679,\n",
            "  'token_str': 'firmy'},\n",
            " {'score': 0.02498946152627468,\n",
            "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do siebie.',\n",
            "  'token': 25584,\n",
            "  'token_str': 'siebie'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.0534038245677948,\n",
              "  'token': 32835,\n",
              "  'token_str': 'niego',\n",
              "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do niego.'},\n",
              " {'score': 0.034703005105257034,\n",
              "  'token': 23393,\n",
              "  'token_str': 'zespołu',\n",
              "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do zespołu.'},\n",
              " {'score': 0.03168296068906784,\n",
              "  'token': 47607,\n",
              "  'token_str': 'siebie',\n",
              "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do siebie.'},\n",
              " {'score': 0.02968348003923893,\n",
              "  'token': 42662,\n",
              "  'token_str': 'produkcji',\n",
              "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do produkcji.'},\n",
              " {'score': 0.02655954845249653,\n",
              "  'token': 38844,\n",
              "  'token_str': 'szkoły',\n",
              "  'sequence': 'Czy ty naprawdę rozbiłeś samochód należący do szkoły.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Nie wierzę że rozbiłeś samochód należący do')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fc86xPBOlrh",
        "outputId": "acf80427-f0ab-4b5c-9a38-e83cd9cec2c7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Nie wierzę że rozbiłeś samochód należący do mnie... To chyba nie miało większego znaczenia...\\nDzięki za ten cytat. Zawsze będę na nie czekał ze wzruszeniem ust, a po zakończeniu wycieczki z powrotem do domu na wszelki wypadek zabrałem go\\nNo więc'}]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celownik:"
      ],
      "metadata": {
        "id": "2-w6ixRLURMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Nie biegaj z nożyczkami bo wydłubiesz temu {nlp.tokenizer.mask_token} oko.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Nie biegaj z nożyczkami bo wydłubiesz temu <mask> oko.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Nie biegaj z nożyczkami bo wydłubiesz temu [MASK] oko.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5xEqW0a6fT_",
        "outputId": "80efb8b1-83cc-4fbc-8bfa-a58bdb36843c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.1370924711227417, 'token': 24207, 'token_str': 'człowiekowi', 'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu człowiekowi oko.'}\n",
            "{'score': 0.08891067653894424, 'token': 19833, 'token_str': 'dziecku', 'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu dziecku oko.'}\n",
            "{'score': 0.08021187782287598, 'token': 49168, 'token_str': 'chłopcu', 'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu chłopcu oko.'}\n",
            "{'score': 0.07516676932573318, 'token': 25907, 'token_str': 'psu', 'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu psu oko.'}\n",
            "{'score': 0.06193472072482109, 'token': 3205, 'token_str': 'dobre', 'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu dobre oko.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.12426617741584778,\n",
            "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu całe oko.',\n",
            "  'token': 122711,\n",
            "  'token_str': 'całe'},\n",
            " {'score': 0.12316421419382095,\n",
            "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu Twoje oko.',\n",
            "  'token': 105902,\n",
            "  'token_str': 'Twoje'},\n",
            " {'score': 0.12104906886816025,\n",
            "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu swoje oko.',\n",
            "  'token': 22220,\n",
            "  'token_str': 'swoje'},\n",
            " {'score': 0.0650072991847992,\n",
            "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu twoje oko.',\n",
            "  'token': 156165,\n",
            "  'token_str': 'twoje'},\n",
            " {'score': 0.049991920590400696,\n",
            "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu jego oko.',\n",
            "  'token': 8658,\n",
            "  'token_str': 'jego'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07052890211343765,\n",
              "  'token': 2532,\n",
              "  'token_str': '刁',\n",
              "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu 刁 oko.'},\n",
              " {'score': 0.05188630521297455,\n",
              "  'token': 10132,\n",
              "  'token_str': 'na',\n",
              "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu na oko.'},\n",
              " {'score': 0.034005582332611084,\n",
              "  'token': 11048,\n",
              "  'token_str': 'przez',\n",
              "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu przez oko.'},\n",
              " {'score': 0.03237557411193848,\n",
              "  'token': 14515,\n",
              "  'token_str': 'tego',\n",
              "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu tego oko.'},\n",
              " {'score': 0.030377833172678947,\n",
              "  'token': 12979,\n",
              "  'token_str': 'ich',\n",
              "  'sequence': 'Nie biegaj z nożyczkami bo wydłubiesz temu ich oko.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Z wielkim zaciekawieniem przyglądał się')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do3Xyy61Ousk",
        "outputId": "e46b1260-31ba-44bc-bb4e-65ca4bfa842f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Z wielkim zaciekawieniem przyglądał się swoim najbliższym współpracownikom, a najbardziej zszokowanym był ten pracujący dla firmy kurierskiej. Nie tylko był bardzo zaciekawiony jej nowym modelem, ale także zauważył kilka jego „braciówek”.\\n„Zaraz”'}]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Biernik:"
      ],
      "metadata": {
        "id": "Ks-iSOWPUTgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Tak naprawdę to wcale nie lubię tego {nlp.tokenizer.mask_token}.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Tak naprawdę to wcale nie lubię tego <mask>.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Tak naprawdę to wcale nie lubię tego [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foTJYpxG6gE2",
        "outputId": "aeb228b9-97e9-4615-d51c-67b65a0b2ad8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.10374286025762558, 'token': 3662, 'token_str': 'słowa', 'sequence': 'Tak naprawdę to wcale nie lubię tego słowa.'}\n",
            "{'score': 0.05642924830317497, 'token': 2165, 'token_str': 'miejsca', 'sequence': 'Tak naprawdę to wcale nie lubię tego miejsca.'}\n",
            "{'score': 0.04927632585167885, 'token': 13701, 'token_str': 'określenia', 'sequence': 'Tak naprawdę to wcale nie lubię tego określenia.'}\n",
            "{'score': 0.043055471032857895, 'token': 11089, 'token_str': 'gościa', 'sequence': 'Tak naprawdę to wcale nie lubię tego gościa.'}\n",
            "{'score': 0.029167260974645615, 'token': 9848, 'token_str': 'faceta', 'sequence': 'Tak naprawdę to wcale nie lubię tego faceta.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.0710945874452591,\n",
            "  'sequence': 'Tak naprawdę to wcale nie lubię tego świata.',\n",
            "  'token': 57528,\n",
            "  'token_str': 'świata'},\n",
            " {'score': 0.0699707493185997,\n",
            "  'sequence': 'Tak naprawdę to wcale nie lubię tego słowa.',\n",
            "  'token': 73743,\n",
            "  'token_str': 'słowa'},\n",
            " {'score': 0.04814814776182175,\n",
            "  'sequence': 'Tak naprawdę to wcale nie lubię tego typu.',\n",
            "  'token': 22177,\n",
            "  'token_str': 'typu'},\n",
            " {'score': 0.03529063239693642,\n",
            "  'sequence': 'Tak naprawdę to wcale nie lubię tego robić.',\n",
            "  'token': 102665,\n",
            "  'token_str': 'robić'},\n",
            " {'score': 0.03445182740688324,\n",
            "  'sequence': 'Tak naprawdę to wcale nie lubię tego języka.',\n",
            "  'token': 106323,\n",
            "  'token_str': 'języka'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.03495929762721062,\n",
              "  'token': 39101,\n",
              "  'token_str': 'rodzaju',\n",
              "  'sequence': 'Tak naprawdę to wcale nie lubię tego rodzaju.'},\n",
              " {'score': 0.03353691101074219,\n",
              "  'token': 44640,\n",
              "  'token_str': 'samego',\n",
              "  'sequence': 'Tak naprawdę to wcale nie lubię tego samego.'},\n",
              " {'score': 0.03218229487538338,\n",
              "  'token': 23330,\n",
              "  'token_str': 'бассейнĕ',\n",
              "  'sequence': 'Tak naprawdę to wcale nie lubię tego бассейнĕ.'},\n",
              " {'score': 0.031112544238567352,\n",
              "  'token': 36636,\n",
              "  'token_str': 'czasu',\n",
              "  'sequence': 'Tak naprawdę to wcale nie lubię tego czasu.'},\n",
              " {'score': 0.030684219673275948,\n",
              "  'token': 20708,\n",
              "  'token_str': 'typu',\n",
              "  'sequence': 'Tak naprawdę to wcale nie lubię tego typu.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Tak naprawdę to wcale nie lubię tego')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKdYHRZqOznX",
        "outputId": "a6a4b45a-d7b7-474f-9ffe-cff4ad2ee839"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Tak naprawdę to wcale nie lubię tego typu tekstów, bo ich autor – niczym się nie wyróżnia i często jest to zupełnie bez sensu. Tak było również z całą historią. Gdy autor postanowił w sposób ciekawy pokazać, że nie można udawać, że wszystko jest w'}]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narzędnik:"
      ],
      "metadata": {
        "id": "M-5-fXzyUWVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Chodzę z {nlp.tokenizer.mask_token} na łańcuchu.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Chodzę z <mask> na łańcuchu.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Chodzę z [MASK] na łańcuchu.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxDVToEI6kay",
        "outputId": "99865683-2484-4aca-d0f3-7615f2257bb1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.1329876035451889, 'token': 12193, 'token_str': 'facetem', 'sequence': 'Chodzę z facetem na łańcuchu.'}\n",
            "{'score': 0.06570342928171158, 'token': 2575, 'token_str': 'tobą', 'sequence': 'Chodzę z tobą na łańcuchu.'}\n",
            "{'score': 0.059189874678850174, 'token': 10006, 'token_str': 'dziewczyną', 'sequence': 'Chodzę z dziewczyną na łańcuchu.'}\n",
            "{'score': 0.057175278663635254, 'token': 28672, 'token_str': 'psem', 'sequence': 'Chodzę z psem na łańcuchu.'}\n",
            "{'score': 0.04654540494084358, 'token': 1742, 'token_str': 'nim', 'sequence': 'Chodzę z nim na łańcuchu.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.2089357078075409,\n",
            "  'sequence': 'Chodzę z dzieckiem na łańcuchu.',\n",
            "  'token': 214006,\n",
            "  'token_str': 'dzieckiem'},\n",
            " {'score': 0.11550019681453705,\n",
            "  'sequence': 'Chodzę z samochodem na łańcuchu.',\n",
            "  'token': 241952,\n",
            "  'token_str': 'samochodem'},\n",
            " {'score': 0.06862897425889969,\n",
            "  'sequence': 'Chodzę z tyłu na łańcuchu.',\n",
            "  'token': 203238,\n",
            "  'token_str': 'tyłu'},\n",
            " {'score': 0.06370428949594498,\n",
            "  'sequence': 'Chodzę z dziećmi na łańcuchu.',\n",
            "  'token': 182176,\n",
            "  'token_str': 'dziećmi'},\n",
            " {'score': 0.038591764867305756,\n",
            "  'sequence': 'Chodzę z nim na łańcuchu.',\n",
            "  'token': 15993,\n",
            "  'token_str': 'nim'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.20702576637268066,\n",
              "  'token': 32658,\n",
              "  'token_str': '##yje',\n",
              "  'sequence': 'Chodzę zyje na łańcuchu.'},\n",
              " {'score': 0.05737800523638725,\n",
              "  'token': 35235,\n",
              "  'token_str': 'преко',\n",
              "  'sequence': 'Chodzę z преко na łańcuchu.'},\n",
              " {'score': 0.051250338554382324,\n",
              "  'token': 43461,\n",
              "  'token_str': '##tón',\n",
              "  'sequence': 'Chodzę ztón na łańcuchu.'},\n",
              " {'score': 0.049201685935258865,\n",
              "  'token': 7319,\n",
              "  'token_str': '访',\n",
              "  'sequence': 'Chodzę z 访 na łańcuchu.'},\n",
              " {'score': 0.04176469147205353,\n",
              "  'token': 14999,\n",
              "  'token_str': 'nich',\n",
              "  'sequence': 'Chodzę z nich na łańcuchu.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Sorry, nie jestem zainteresowana, chodzę z')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJWnhS2ZO8Nf",
        "outputId": "50aafdf1-b443-48c5-866e-f9a022b090ee"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Sorry, nie jestem zainteresowana, chodzę z dwoma szwami i do tego mam jeden zamek, a do tego jeszcze muszę założyć na klapę, a ja i tak jeszcze nie wiem co w niej będzie. Nie wiem, że to wszystko ma mi'}]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Miejscownik:"
      ],
      "metadata": {
        "id": "RnnovPNdUY6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Opowiem ci bajkę o {nlp.tokenizer.mask_token} na łańcuchu.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Opowiem ci bajkę o <mask> na łańcuchu.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Opowiem ci bajkę o [MASK] na łańcuchu.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGIkBDyT6lZU",
        "outputId": "2907c838-9f2c-4f40-ea20-0d19a9639e66"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.15143153071403503, 'token': 59238, 'token_str': 'facecie', 'sequence': 'Opowiem ci bajkę o facecie na łańcuchu.'}\n",
            "{'score': 0.12364137917757034, 'token': 10483, 'token_str': 'człowieku', 'sequence': 'Opowiem ci bajkę o człowieku na łańcuchu.'}\n",
            "{'score': 0.0645129457116127, 'token': 23319, 'token_str': 'dziewczynie', 'sequence': 'Opowiem ci bajkę o dziewczynie na łańcuchu.'}\n",
            "{'score': 0.053697504103183746, 'token': 24019, 'token_str': 'kobiecie', 'sequence': 'Opowiem ci bajkę o kobiecie na łańcuchu.'}\n",
            "{'score': 0.04928913712501526, 'token': 37174, 'token_str': 'koniu', 'sequence': 'Opowiem ci bajkę o koniu na łańcuchu.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.2123054713010788,\n",
            "  'sequence': 'Opowiem ci bajkę o życiu na łańcuchu.',\n",
            "  'token': 71942,\n",
            "  'token_str': 'życiu'},\n",
            " {'score': 0.07730427384376526,\n",
            "  'sequence': 'Opowiem ci bajkę o miłości na łańcuchu.',\n",
            "  'token': 133472,\n",
            "  'token_str': 'miłości'},\n",
            " {'score': 0.051993172615766525,\n",
            "  'sequence': 'Opowiem ci bajkę o śmierci na łańcuchu.',\n",
            "  'token': 124795,\n",
            "  'token_str': 'śmierci'},\n",
            " {'score': 0.05063553899526596,\n",
            "  'sequence': 'Opowiem ci bajkę o podróży na łańcuchu.',\n",
            "  'token': 147564,\n",
            "  'token_str': 'podróży'},\n",
            " {'score': 0.03577881678938866,\n",
            "  'sequence': 'Opowiem ci bajkę o pracy na łańcuchu.',\n",
            "  'token': 11272,\n",
            "  'token_str': 'pracy'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07080040872097015,\n",
              "  'token': 39447,\n",
              "  'token_str': 'sobie',\n",
              "  'sequence': 'Opowiem ci bajkę o sobie na łańcuchu.'},\n",
              " {'score': 0.035769544541835785,\n",
              "  'token': 57437,\n",
              "  'token_str': 'pomoc',\n",
              "  'sequence': 'Opowiem ci bajkę o pomoc na łańcuchu.'},\n",
              " {'score': 0.026514628902077675,\n",
              "  'token': 51680,\n",
              "  'token_str': 'ziemi',\n",
              "  'sequence': 'Opowiem ci bajkę o ziemi na łańcuchu.'},\n",
              " {'score': 0.024922646582126617,\n",
              "  'token': 18471,\n",
              "  'token_str': '##선',\n",
              "  'sequence': 'Opowiem ci bajkę o선 na łańcuchu.'},\n",
              " {'score': 0.01881132833659649,\n",
              "  'token': 12644,\n",
              "  'token_str': 'tym',\n",
              "  'sequence': 'Opowiem ci bajkę o tym na łańcuchu.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Znowu gadaliście o')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugBIcMWkPDXU",
        "outputId": "bbd58616-b823-4601-f61e-c161abbe78d5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Znowu gadaliście o tym, że w Japonii panuje moda na sushi (tak, tak!) i na sushi, że w Polsce jest na prawdę szał. To nieprawda, za 5 lat będziemy mieli sushi z Chin.\\nMy będziemy mieli z Japonii!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wołacz:"
      ],
      "metadata": {
        "id": "vbKDR7F2Ubcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Witaj w domu, mój {nlp.tokenizer.mask_token}.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Witaj w domu, mój <mask>.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Witaj w domu, mój [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q6joCTu6mTR",
        "outputId": "e7547be3-c4e1-456f-9589-77ffe884fa0d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.2157355546951294, 'token': 11117, 'token_str': 'synu', 'sequence': 'Witaj w domu, mój synu.'}\n",
            "{'score': 0.16066007316112518, 'token': 15273, 'token_str': 'przyjacielu', 'sequence': 'Witaj w domu, mój przyjacielu.'}\n",
            "{'score': 0.1274672895669937, 'token': 16435, 'token_str': 'chłopcze', 'sequence': 'Witaj w domu, mój chłopcze.'}\n",
            "{'score': 0.09302274137735367, 'token': 29235, 'token_str': 'kochany', 'sequence': 'Witaj w domu, mój kochany.'}\n",
            "{'score': 0.05750471353530884, 'token': 2378, 'token_str': 'panie', 'sequence': 'Witaj w domu, mój panie.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.08882402628660202,\n",
            "  'sequence': 'Witaj w domu, mój dom.',\n",
            "  'token': 4539,\n",
            "  'token_str': 'dom'},\n",
            " {'score': 0.07899118214845657,\n",
            "  'sequence': 'Witaj w domu, mój tata.',\n",
            "  'token': 32901,\n",
            "  'token_str': 'tata'},\n",
            " {'score': 0.07053118199110031,\n",
            "  'sequence': 'Witaj w domu, mój przyjaciel.',\n",
            "  'token': 184158,\n",
            "  'token_str': 'przyjaciel'},\n",
            " {'score': 0.037776537239551544,\n",
            "  'sequence': 'Witaj w domu, mój drogi.',\n",
            "  'token': 96020,\n",
            "  'token_str': 'drogi'},\n",
            " {'score': 0.031911302357912064,\n",
            "  'sequence': 'Witaj w domu, mój kocha.',\n",
            "  'token': 77420,\n",
            "  'token_str': 'kocha'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.06373664736747742,\n",
              "  'token': 59311,\n",
              "  'token_str': 'ojciec',\n",
              "  'sequence': 'Witaj w domu, mój ojciec.'},\n",
              " {'score': 0.0451982244849205,\n",
              "  'token': 31838,\n",
              "  'token_str': 'dzieci',\n",
              "  'sequence': 'Witaj w domu, mój dzieci.'},\n",
              " {'score': 0.035685501992702484,\n",
              "  'token': 28655,\n",
              "  'token_str': '২০০১',\n",
              "  'sequence': 'Witaj w domu, mój ২০০১.'},\n",
              " {'score': 0.013670919463038445,\n",
              "  'token': 27742,\n",
              "  'token_str': 'domu',\n",
              "  'sequence': 'Witaj w domu, mój domu.'},\n",
              " {'score': 0.011937716044485569,\n",
              "  'token': 39449,\n",
              "  'token_str': 'brat',\n",
              "  'sequence': 'Witaj w domu, mój brat.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Witaj w domu, mój')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkXX7MtkPJ10",
        "outputId": "528a9ed7-c426-4529-ddbc-0a5fd9fcf812"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Witaj w domu, mój drogi Arkuszu, ale teraz nie można używać tego zegara. Nie mogę używać klawiszy \"Zanussi\". Jak działa zegar z tym zegarem? Jak mam to połączyć?\\nCzy da się z tym zegarkiem połączyć zegar'}]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 4: Sprawdzanie czy modele uchwycą odległe relacje między słowami, takie jak płeć"
      ],
      "metadata": {
        "id": "Bz2QO-iTTw5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Podeszła i {nlp.tokenizer.mask_token} go w policzek.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Podeszła i <mask> go w policzek.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Podeszła i [MASK] go w policzek.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cusn6jLXypLy",
        "outputId": "d8c648a0-216c-483f-a7e9-104702c20725"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.9081022143363953, 'token': 41686, 'token_str': 'uderzyła', 'sequence': 'Podeszła i uderzyła go w policzek.'}\n",
            "{'score': 0.021244468167424202, 'token': 38635, 'token_str': 'rzuciła', 'sequence': 'Podeszła i rzuciła go w policzek.'}\n",
            "{'score': 0.02008851431310177, 'token': 19169, 'token_str': 'dostała', 'sequence': 'Podeszła i dostała go w policzek.'}\n",
            "{'score': 0.01732712797820568, 'token': 23167, 'token_str': 'trafiła', 'sequence': 'Podeszła i trafiła go w policzek.'}\n",
            "{'score': 0.004773114807903767, 'token': 58333, 'token_str': 'dotknęła', 'sequence': 'Podeszła i dotknęła go w policzek.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.3044286370277405,\n",
            "  'sequence': 'Podeszła i dala go w policzek.',\n",
            "  'token': 43004,\n",
            "  'token_str': 'dala'},\n",
            " {'score': 0.14276286959648132,\n",
            "  'sequence': 'Podeszła i pani go w policzek.',\n",
            "  'token': 20381,\n",
            "  'token_str': 'pani'},\n",
            " {'score': 0.09327396750450134,\n",
            "  'sequence': 'Podeszła i miała go w policzek.',\n",
            "  'token': 76293,\n",
            "  'token_str': 'miała'},\n",
            " {'score': 0.049512870609760284,\n",
            "  'sequence': 'Podeszła i chciała go w policzek.',\n",
            "  'token': 166929,\n",
            "  'token_str': 'chciała'},\n",
            " {'score': 0.02561912126839161,\n",
            "  'sequence': 'Podeszła i trzyma go w policzek.',\n",
            "  'token': 99084,\n",
            "  'token_str': 'trzyma'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.039678383618593216,\n",
              "  'token': 3276,\n",
              "  'token_str': '姜',\n",
              "  'sequence': 'Podeszła i 姜 go w policzek.'},\n",
              " {'score': 0.03501329571008682,\n",
              "  'token': 19189,\n",
              "  'token_str': 'znajduje',\n",
              "  'sequence': 'Podeszła i znajduje go w policzek.'},\n",
              " {'score': 0.02117384597659111,\n",
              "  'token': 60947,\n",
              "  'token_str': 'prowadził',\n",
              "  'sequence': 'Podeszła i prowadził go w policzek.'},\n",
              " {'score': 0.01972079463303089,\n",
              "  'token': 37489,\n",
              "  'token_str': 'Đảng',\n",
              "  'sequence': 'Podeszła i Đảng go w policzek.'},\n",
              " {'score': 0.016608284786343575,\n",
              "  'token': 29377,\n",
              "  'token_str': 'ponownie',\n",
              "  'sequence': 'Podeszła i ponownie go w policzek.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Zrobiła krok w tył, po czym')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP7Zz72fPTdp",
        "outputId": "1e381d7a-d4eb-419f-8101-4c42564d350c"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Zrobiła krok w tył, po czym z trudem się wyprostowała. Spojrzała na Megan. Jej twarz była cała czerwona, a włosy przysłaniała tylko maska. Była zmęczona. Z drugiej strony była kobietą, która musiała być kobietą pełną życia.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Wziął głęboki wdech i {nlp.tokenizer.mask_token} poemat.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Wziął głęboki wdech i <mask> poemat.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Wziął głęboki wdech i [MASK] poemat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM6xqeUx_6aw",
        "outputId": "7c8faae4-be62-4cfb-caaf-005e0212d8e1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.6942387819290161, 'token': 9134, 'token_str': 'napisał', 'sequence': 'Wziął głęboki wdech i napisał poemat.'}\n",
            "{'score': 0.13171416521072388, 'token': 52154, 'token_str': 'skomponował', 'sequence': 'Wziął głęboki wdech i skomponował poemat.'}\n",
            "{'score': 0.021030357107520103, 'token': 7926, 'token_str': 'ukończył', 'sequence': 'Wziął głęboki wdech i ukończył poemat.'}\n",
            "{'score': 0.016876254230737686, 'token': 10928, 'token_str': 'stworzył', 'sequence': 'Wziął głęboki wdech i stworzył poemat.'}\n",
            "{'score': 0.010219919495284557, 'token': 35644, 'token_str': 'wygłosił', 'sequence': 'Wziął głęboki wdech i wygłosił poemat.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.1447429358959198,\n",
            "  'sequence': 'Wziął głęboki wdech i napisał poemat.',\n",
            "  'token': 118629,\n",
            "  'token_str': 'napisał'},\n",
            " {'score': 0.09289433062076569,\n",
            "  'sequence': 'Wziął głęboki wdech i rozpoczął poemat.',\n",
            "  'token': 240953,\n",
            "  'token_str': 'rozpoczął'},\n",
            " {'score': 0.08221791684627533,\n",
            "  'sequence': 'Wziął głęboki wdech i piękny poemat.',\n",
            "  'token': 120229,\n",
            "  'token_str': 'piękny'},\n",
            " {'score': 0.0799727663397789,\n",
            "  'sequence': 'Wziął głęboki wdech i powstał poemat.',\n",
            "  'token': 69111,\n",
            "  'token_str': 'powstał'},\n",
            " {'score': 0.05329102650284767,\n",
            "  'sequence': 'Wziął głęboki wdech i zakończył poemat.',\n",
            "  'token': 208075,\n",
            "  'token_str': 'zakończył'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07820765674114227,\n",
              "  'token': 27976,\n",
              "  'token_str': 'برخی',\n",
              "  'sequence': 'Wziął głęboki wdech i برخی poemat.'},\n",
              " {'score': 0.07184188812971115,\n",
              "  'token': 11769,\n",
              "  'token_str': 'ten',\n",
              "  'sequence': 'Wziął głęboki wdech i ten poemat.'},\n",
              " {'score': 0.06911905854940414,\n",
              "  'token': 2532,\n",
              "  'token_str': '刁',\n",
              "  'sequence': 'Wziął głęboki wdech i 刁 poemat.'},\n",
              " {'score': 0.034545425325632095,\n",
              "  'token': 64751,\n",
              "  'token_str': 'zawiera',\n",
              "  'sequence': 'Wziął głęboki wdech i zawiera poemat.'},\n",
              " {'score': 0.03281639888882637,\n",
              "  'token': 12412,\n",
              "  'token_str': 'jego',\n",
              "  'sequence': 'Wziął głęboki wdech i jego poemat.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Wziął głęboki wdech, a następnie')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDGxGa__PZCe",
        "outputId": "8bb785b2-aa7a-4029-a4ee-5130c5fe852f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Wziął głęboki wdech, a następnie odrywając ręce od rękojeści, przeczesał włosy palcami. Z jego czoła wydarto mu błysk...\\n- Nic, tylko musisz wziąć ten cały ciężar i wrócić tutaj - powiedział,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"Raz się zobaczyli i od razu się {nlp.tokenizer.mask_token}.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"Raz się zobaczyli i od razu się <mask>.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"Raz się zobaczyli i od razu się [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUvCkjKBADkM",
        "outputId": "83e9b2a8-0691-4aed-e8c3-d0226399accb"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.20473983883857727, 'token': 25359, 'token_str': 'spotkali', 'sequence': 'Raz się zobaczyli i od razu się spotkali.'}\n",
            "{'score': 0.08433767408132553, 'token': 43402, 'token_str': 'bili', 'sequence': 'Raz się zobaczyli i od razu się bili.'}\n",
            "{'score': 0.06944000720977783, 'token': 35510, 'token_str': 'dowiedzieli', 'sequence': 'Raz się zobaczyli i od razu się dowiedzieli.'}\n",
            "{'score': 0.06700269132852554, 'token': 40373, 'token_str': 'zobaczyli', 'sequence': 'Raz się zobaczyli i od razu się zobaczyli.'}\n",
            "{'score': 0.06613392382860184, 'token': 36453, 'token_str': 'pojawili', 'sequence': 'Raz się zobaczyli i od razu się pojawili.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.2620030343532562,\n",
            "  'sequence': 'Raz się zobaczyli i od razu się udal.',\n",
            "  'token': 65468,\n",
            "  'token_str': 'udal'},\n",
            " {'score': 0.22247447073459625,\n",
            "  'sequence': 'Raz się zobaczyli i od razu się dali.',\n",
            "  'token': 53616,\n",
            "  'token_str': 'dali'},\n",
            " {'score': 0.0475660003721714,\n",
            "  'sequence': 'Raz się zobaczyli i od razu się stali.',\n",
            "  'token': 78000,\n",
            "  'token_str': 'stali'},\n",
            " {'score': 0.025288822129368782,\n",
            "  'sequence': 'Raz się zobaczyli i od razu się spotkał.',\n",
            "  'token': 218580,\n",
            "  'token_str': 'spotkał'},\n",
            " {'score': 0.02190803736448288,\n",
            "  'sequence': 'Raz się zobaczyli i od razu się wali.',\n",
            "  'token': 13019,\n",
            "  'token_str': 'wali'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.048650939017534256,\n",
              "  'token': 46537,\n",
              "  'token_str': '##owali',\n",
              "  'sequence': 'Raz się zobaczyli i od razu sięowali.'},\n",
              " {'score': 0.023626379668712616,\n",
              "  'token': 10390,\n",
              "  'token_str': '##li',\n",
              "  'sequence': 'Raz się zobaczyli i od razu sięli.'},\n",
              " {'score': 0.021140582859516144,\n",
              "  'token': 9631,\n",
              "  'token_str': '윽',\n",
              "  'sequence': 'Raz się zobaczyli i od razu się 윽.'},\n",
              " {'score': 0.0195388775318861,\n",
              "  'token': 30231,\n",
              "  'token_str': 'oni',\n",
              "  'sequence': 'Raz się zobaczyli i od razu się oni.'},\n",
              " {'score': 0.017275698482990265,\n",
              "  'token': 19887,\n",
              "  'token_str': 'byli',\n",
              "  'sequence': 'Raz się zobaczyli i od razu się byli.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Raz się zobaczyli i od razu się')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLL12GTKPvy-",
        "outputId": "9b560f3c-0a6a-4a6d-e45a-76589f232bd9"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Raz się zobaczyli i od razu się nie widzieli, więc się nie może w żaden sposób tłumaczyć. Ale tak na poważnie to wszyscy już dawno byli razem i może dlatego tak ich sobie wymyśliłam. No dobrze, jestem głupia. To może po prostu na'}]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 5: Sprawdzanie czy modele są w stanie uchwycić wiedzę na temat świata"
      ],
      "metadata": {
        "id": "YF3kjVH_ToUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"{nlp.tokenizer.mask_token} wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"<mask> wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykuAAGb2KjX4",
        "outputId": "0bfbed8e-1aac-4e86-fbd4-5b8a92326fa8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.1671256572008133, 'token': 19776, 'token_str': 'Woda', 'sequence': 'Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}\n",
            "{'score': 0.06379453092813492, 'token': 59321, 'token_str': 'Mięso', 'sequence': 'Mięso wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}\n",
            "{'score': 0.06294772773981094, 'token': 23753, 'token_str': 'Słońce', 'sequence': 'Słońce wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}\n",
            "{'score': 0.05818343535065651, 'token': 835, 'token_str': 'Nie', 'sequence': 'Nie wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}\n",
            "{'score': 0.03854730352759361, 'token': 17349, 'token_str': 'Ziemia', 'sequence': 'Ziemia wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.11466275900602341,\n",
            "  'sequence': 'Najlepiej wrze w temperaturze 100 stopni, a zamarza w '\n",
            "              'temperaturze 0 stopni Celsjusza.',\n",
            "  'token': 229067,\n",
            "  'token_str': 'Najlepiej'},\n",
            " {'score': 0.10112470388412476,\n",
            "  'sequence': '- wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 '\n",
            "              'stopni Celsjusza.',\n",
            "  'token': 20,\n",
            "  'token_str': '-'},\n",
            " {'score': 0.04047984629869461,\n",
            "  'sequence': 'Najczęściej wrze w temperaturze 100 stopni, a zamarza w '\n",
            "              'temperaturze 0 stopni Celsjusza.',\n",
            "  'token': 239944,\n",
            "  'token_str': 'Najczęściej'},\n",
            " {'score': 0.03635406494140625,\n",
            "  'sequence': '– wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 '\n",
            "              'stopni Celsjusza.',\n",
            "  'token': 46,\n",
            "  'token_str': '–'},\n",
            " {'score': 0.03353913128376007,\n",
            "  'sequence': 'Następnie wrze w temperaturze 100 stopni, a zamarza w '\n",
            "              'temperaturze 0 stopni Celsjusza.',\n",
            "  'token': 167479,\n",
            "  'token_str': 'Następnie'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.08812884241342545,\n",
              "  'token': 10685,\n",
              "  'token_str': 'Na',\n",
              "  'sequence': 'Na wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'},\n",
              " {'score': 0.049292851239442825,\n",
              "  'token': 12865,\n",
              "  'token_str': 'We',\n",
              "  'sequence': 'We wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'},\n",
              " {'score': 0.03352230042219162,\n",
              "  'token': 11936,\n",
              "  'token_str': 'Od',\n",
              "  'sequence': 'Od wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'},\n",
              " {'score': 0.03250037878751755,\n",
              "  'token': 160,\n",
              "  'token_str': 'W',\n",
              "  'sequence': 'W wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'},\n",
              " {'score': 0.029776066541671753,\n",
              "  'token': 11951,\n",
              "  'token_str': 'we',\n",
              "  'sequence': 'we wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('W temperaturze 100 stopni celsjusza gotuje się')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xjYR-LvP6L3",
        "outputId": "01e52e94-0224-463a-9b28-48c7717a2b32"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'W temperaturze 100 stopni celsjusza gotuje się już tylko pod przykryciem w temp 170-190 stopni i nic nie czuć. Próbowałem go podgrzewać w piekarniku przez 90 minut w 20 stopni przez 50 minut w 18 stopni przez 20 minut, teraz mam 180 stopni'}]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"{nlp.tokenizer.mask_token} są statystycznie wyżsi od kobiet.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"<mask> są statystycznie wyżsi od kobiet.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"[MASK] są statystycznie wyżsi od kobiet.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEThUyE2LTbi",
        "outputId": "fc5424ef-b2fe-41e3-b4bd-88526e65ebdf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.9192365407943726, 'token': 30392, 'token_str': 'Mężczyźni', 'sequence': 'Mężczyźni są statystycznie wyżsi od kobiet.'}\n",
            "{'score': 0.018977688625454903, 'token': 5504, 'token_str': 'Ludzie', 'sequence': 'Ludzie są statystycznie wyżsi od kobiet.'}\n",
            "{'score': 0.008311716839671135, 'token': 12766, 'token_str': 'Polacy', 'sequence': 'Polacy są statystycznie wyżsi od kobiet.'}\n",
            "{'score': 0.005962057504802942, 'token': 18175, 'token_str': 'Kobiety', 'sequence': 'Kobiety są statystycznie wyżsi od kobiet.'}\n",
            "{'score': 0.004151514731347561, 'token': 22316, 'token_str': 'Amerykanie', 'sequence': 'Amerykanie są statystycznie wyżsi od kobiet.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.950264036655426,\n",
            "  'sequence': 'mężczyźni są statystycznie wyżsi od kobiet.',\n",
            "  'token': 233782,\n",
            "  'token_str': 'mężczyźni'},\n",
            " {'score': 0.016299257054924965,\n",
            "  'sequence': 'mężczyzn są statystycznie wyżsi od kobiet.',\n",
            "  'token': 71941,\n",
            "  'token_str': 'mężczyzn'},\n",
            " {'score': 0.01452503353357315,\n",
            "  'sequence': 'Polacy są statystycznie wyżsi od kobiet.',\n",
            "  'token': 214648,\n",
            "  'token_str': 'Polacy'},\n",
            " {'score': 0.004106596577912569,\n",
            "  'sequence': 'Młodzi są statystycznie wyżsi od kobiet.',\n",
            "  'token': 197453,\n",
            "  'token_str': 'Młodzi'},\n",
            " {'score': 0.0024810757022351027,\n",
            "  'sequence': 'Oni są statystycznie wyżsi od kobiet.',\n",
            "  'token': 40332,\n",
            "  'token_str': 'Oni'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.17023174464702606,\n",
              "  'token': 35450,\n",
              "  'token_str': 'Obecnie',\n",
              "  'sequence': 'Obecnie są statystycznie wyżsi od kobiet.'},\n",
              " {'score': 0.1098800078034401,\n",
              "  'token': 42122,\n",
              "  'token_str': 'popolasiù',\n",
              "  'sequence': 'popolasiù są statystycznie wyżsi od kobiet.'},\n",
              " {'score': 0.05802447348833084,\n",
              "  'token': 25879,\n",
              "  'token_str': 'Nie',\n",
              "  'sequence': 'Nie są statystycznie wyżsi od kobiet.'},\n",
              " {'score': 0.021661968901753426,\n",
              "  'token': 2171,\n",
              "  'token_str': '享',\n",
              "  'sequence': '享 są statystycznie wyżsi od kobiet.'},\n",
              " {'score': 0.021019499748945236,\n",
              "  'token': 26637,\n",
              "  'token_str': 'Stan',\n",
              "  'sequence': 'Stan są statystycznie wyżsi od kobiet.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Statystycznie wyżsi od kobiet są')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7TWtkBJQNn-",
        "outputId": "105ed2d4-30b6-4131-b15b-cec53a2ee70c"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Statystycznie wyżsi od kobiet są też mężczyźni. W zależności od pozycji społecznej są bardziej i mniej odporni na choroby. Jak wynika z danych epidemiologicznych i obserwacji kobiet na całym świecie aż 50 proc. przypadków grypy stanowi kobieta.\\nBak'}]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"{nlp.tokenizer.mask_token} posiada dwa księżyce i jest czwartą planetą od Słońca.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"<mask> posiada dwa księżyce i jest czwartą planetą od Słońca.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"[MASK] posiada dwa księżyce i jest czwartą planetą od Słońca.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfvhAdO6L1al",
        "outputId": "35b2f059-d64b-4d99-da9e-13c421f900e6"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.35776862502098083, 'token': 17349, 'token_str': 'Ziemia', 'sequence': 'Ziemia posiada dwa księżyce i jest czwartą planetą od Słońca.'}\n",
            "{'score': 0.12897801399230957, 'token': 23753, 'token_str': 'Słońce', 'sequence': 'Słońce posiada dwa księżyce i jest czwartą planetą od Słońca.'}\n",
            "{'score': 0.06586483865976334, 'token': 14443, 'token_str': 'Księżyc', 'sequence': 'Księżyc posiada dwa księżyce i jest czwartą planetą od Słońca.'}\n",
            "{'score': 0.04074924439191818, 'token': 41237, 'token_str': 'Wenus', 'sequence': 'Wenus posiada dwa księżyce i jest czwartą planetą od Słońca.'}\n",
            "{'score': 0.03443901613354683, 'token': 26892, 'token_str': 'Mars', 'sequence': 'Mars posiada dwa księżyce i jest czwartą planetą od Słońca.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.5774120688438416,\n",
            "  'sequence': 'Mars posiada dwa księżyce i jest czwartą planetą od Słońca.',\n",
            "  'token': 23342,\n",
            "  'token_str': 'Mars'},\n",
            " {'score': 0.07911605387926102,\n",
            "  'sequence': 'Saturn posiada dwa księżyce i jest czwartą planetą od Słońca.',\n",
            "  'token': 118651,\n",
            "  'token_str': 'Saturn'},\n",
            " {'score': 0.05634403973817825,\n",
            "  'sequence': 'Terra posiada dwa księżyce i jest czwartą planetą od Słońca.',\n",
            "  'token': 21432,\n",
            "  'token_str': 'Terra'},\n",
            " {'score': 0.033912450075149536,\n",
            "  'sequence': 'Luna posiada dwa księżyce i jest czwartą planetą od Słońca.',\n",
            "  'token': 42873,\n",
            "  'token_str': 'Luna'},\n",
            " {'score': 0.031223619356751442,\n",
            "  'sequence': 'Planet posiada dwa księżyce i jest czwartą planetą od Słońca.',\n",
            "  'token': 40357,\n",
            "  'token_str': 'Planet'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.08657378703355789,\n",
              "  'token': 28058,\n",
              "  'token_str': 'Jupiter',\n",
              "  'sequence': 'Jupiter posiada dwa księżyce i jest czwartą planetą od Słońca.'},\n",
              " {'score': 0.07873902469873428,\n",
              "  'token': 15205,\n",
              "  'token_str': 'Planet',\n",
              "  'sequence': 'Planet posiada dwa księżyce i jest czwartą planetą od Słońca.'},\n",
              " {'score': 0.05914768949151039,\n",
              "  'token': 51596,\n",
              "  'token_str': 'Orion',\n",
              "  'sequence': 'Orion posiada dwa księżyce i jest czwartą planetą od Słońca.'},\n",
              " {'score': 0.055083151906728745,\n",
              "  'token': 61814,\n",
              "  'token_str': 'Pluto',\n",
              "  'sequence': 'Pluto posiada dwa księżyce i jest czwartą planetą od Słońca.'},\n",
              " {'score': 0.036327484995126724,\n",
              "  'token': 42215,\n",
              "  'token_str': 'Ariel',\n",
              "  'sequence': 'Ariel posiada dwa księżyce i jest czwartą planetą od Słońca.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Wokół Ziemi orbituje ciało niebieskie zwane')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJGXT-qZQVVf",
        "outputId": "776e5a4d-6f35-45a2-9aa4-7ef1b3981094"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Wokół Ziemi orbituje ciało niebieskie zwane plazmą. Ciała niebieskie krążą wokół Słońca a atmosfera Ziemi ma masę równą zeru. Ziemia jest także jednym z trzech ciał ziemskich w kosmosie.\\nNa Ziemi znajduje się jedna z pięciu zamieszkałych planet.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model 1\\n\")\n",
        "for pred in nlp(f\"{nlp.tokenizer.mask_token} pokonali krzyżaków pod Grunwaldem w roku 1410.\"):\n",
        "    print(pred)\n",
        "\n",
        "print(\"\\nModel 2\\n\")\n",
        "pprint.pprint(unmasker_m2(\"<mask> pokonali krzyżaków pod Grunwaldem w roku 1410.\"))\n",
        "\n",
        "print(\"\\nModel 3\\n\")\n",
        "unmasker_m3(\"[MASK] pokonali krzyżaków pod Grunwaldem w roku 1410.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONHBu5XeNFkB",
        "outputId": "f49e3876-27f2-4fc4-f656-d0ccfd921459"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "\n",
            "{'score': 0.5343974232673645, 'token': 12766, 'token_str': 'Polacy', 'sequence': 'Polacy pokonali krzyżaków pod Grunwaldem w roku 1410.'}\n",
            "{'score': 0.1214151605963707, 'token': 58501, 'token_str': 'Szwedzi', 'sequence': 'Szwedzi pokonali krzyżaków pod Grunwaldem w roku 1410.'}\n",
            "{'score': 0.06364317238330841, 'token': 6011, 'token_str': 'Niemcy', 'sequence': 'Niemcy pokonali krzyżaków pod Grunwaldem w roku 1410.'}\n",
            "{'score': 0.05639376491308212, 'token': 22582, 'token_str': 'Rosjanie', 'sequence': 'Rosjanie pokonali krzyżaków pod Grunwaldem w roku 1410.'}\n",
            "{'score': 0.05312406271696091, 'token': 35074, 'token_str': 'Wspólnie', 'sequence': 'Wspólnie pokonali krzyżaków pod Grunwaldem w roku 1410.'}\n",
            "\n",
            "Model 2\n",
            "\n",
            "[{'score': 0.4890036880970001,\n",
            "  'sequence': 'Polacy pokonali krzyżaków pod Grunwaldem w roku 1410.',\n",
            "  'token': 214648,\n",
            "  'token_str': 'Polacy'},\n",
            " {'score': 0.3555125594139099,\n",
            "  'sequence': 'Niemcy pokonali krzyżaków pod Grunwaldem w roku 1410.',\n",
            "  'token': 224030,\n",
            "  'token_str': 'Niemcy'},\n",
            " {'score': 0.04017479345202446,\n",
            "  'sequence': 'Niektórzy pokonali krzyżaków pod Grunwaldem w roku 1410.',\n",
            "  'token': 238214,\n",
            "  'token_str': 'Niektórzy'},\n",
            " {'score': 0.02687974087893963,\n",
            "  'sequence': 'Jak pokonali krzyżaków pod Grunwaldem w roku 1410.',\n",
            "  'token': 4422,\n",
            "  'token_str': 'Jak'},\n",
            " {'score': 0.009073405526578426,\n",
            "  'sequence': 'Gdy pokonali krzyżaków pod Grunwaldem w roku 1410.',\n",
            "  'token': 62926,\n",
            "  'token_str': 'Gdy'}]\n",
            "\n",
            "Model 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6631202697753906,\n",
              "  'token': 53689,\n",
              "  'token_str': 'Niemcy',\n",
              "  'sequence': 'Niemcy pokonali krzyżaków pod Grunwaldem w roku 1410.'},\n",
              " {'score': 0.0347200408577919,\n",
              "  'token': 62733,\n",
              "  'token_str': 'Miasto',\n",
              "  'sequence': 'Miasto pokonali krzyżaków pod Grunwaldem w roku 1410.'},\n",
              " {'score': 0.029427366331219673,\n",
              "  'token': 25879,\n",
              "  'token_str': 'Nie',\n",
              "  'sequence': 'Nie pokonali krzyżaków pod Grunwaldem w roku 1410.'},\n",
              " {'score': 0.015040481463074684,\n",
              "  'token': 58541,\n",
              "  'token_str': 'Parafia',\n",
              "  'sequence': 'Parafia pokonali krzyżaków pod Grunwaldem w roku 1410.'},\n",
              " {'score': 0.00533875310793519,\n",
              "  'token': 31716,\n",
              "  'token_str': 'Polska',\n",
              "  'sequence': 'Polska pokonali krzyżaków pod Grunwaldem w roku 1410.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model 4\\n\")\n",
        "generator('Krzyżacy pod Grunwaldem w 1410 zostali pokonani przez')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJFKrZzTQiCk",
        "outputId": "b8a49751-c1ae-4bef-d426-1b880e76aa96"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Krzyżacy pod Grunwaldem w 1410 zostali pokonani przez Krzyżaków. W drodze powrotnej do Gniezna zatrzymali się między innymi w Toruniu, gdzie można je było oglądać z przewodnikiem.\\nZapisy do turnieju trwały od 5-8 grudnia 2012 r'}]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 7: odpowiedź na pytania:\n",
        ".\n",
        "\n",
        "Krótkie przypomnienie, że naszymi modelami są:\n",
        "\n",
        "Model 1 - dkleczek/bert-base-polish-cased-v1\n",
        "\n",
        "Model 2 - xlm-roberta-base\n",
        "\n",
        "Model 3 - distilbert-base-multilingual-cased\n",
        "\n",
        "Model 4 - flax-community/papuGaPT2 (model dodatkowy)\n",
        "\n",
        ".\n",
        "### I. Który z modeli zwrócił najlepsze rezultaty?\n",
        "Patrząc na ogół wyników to model 1 zwrócił najlepsze rezultaty dla porównywanych zdań. Jego predykcje były najbardziej naturalne, podczas gdy w dwóch pozostałych zdarzało się, że albo były niepoprawne gramatycznie, albo wręcz były to słowa zupełnie nie związane z kontekstem. Model 3 docierając do dna kopał jeszcze głębiej, ponieważ nie radził sobie niekiedy z rozpoznaniem z jakim językiem ma do czynienia.\n",
        "### II. Czy którykolwiek z modeli był w stanie uchwycić polską gramatykę?\n",
        "Tak, przede wszystkim pierwszy model sobie z tym bardzo dobrze poradził. Również PapuGa zauważał polską gramatykę. Pozostałe modele czasem to robiły, czasem nie, więc trudno powiedzieć czy było to szczęście przy poprawnych predykcjach, czy tylko pech przy błędnych.\n",
        "### III. Czy którykolwiek z modeli był w stanie uchwycić odległe relacje między słowami?\n",
        "Model 1, 2 i PapuGa dobrze sobie z tym radziły. Model 3 już nie tak bardzo, ale z nim generalnie coś było nie tak.\n",
        "### IV. Czy którykolwiek z modeli był w stanie uchwycić wiedzę na temat świata?\n",
        "Oczywiście model 1 to potrafił. Co ciekawe w tym wypadku popisał się model 2, który chociażby lepiej przewidział Marsa dla czwartej planety od słońca z dwoma księżycami lub w przypadku mężczyzn statystycznie wyższych od kobiet.\n",
        "### V. Jakie są najbardziej uderzające błędy modeli?\n",
        "Z powodu modelu 3 takimi błędami wydaje mi się wplatanie obcojęzycznych słów do wypełnianego zdania. A już w ogóle uderzające jest, że słowa te pochodzą z języków posługujących się całkiem odmiennymi alfabetami."
      ],
      "metadata": {
        "id": "0QX5fpKlQzHB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-NU_we7TjV5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}